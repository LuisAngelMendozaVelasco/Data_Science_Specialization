---
title: "swirl Lesson 1: Variance Inflation Factors"
output: html_notebook
---

Variance Inflation Factors. (Slides for this and other Data Science courses may be found at github https://github.com/DataScienceSpecialization/courses. If you care to use them, they must be downloaded as a zip file and viewed locally. This lesson corresponds to Regression_Models/02_04_residuals_variation_diagnostics.)

In modeling, our interest lies in parsimonious, interpretable representations of the data that enhance our understanding of the phenomena under study. Omitting variables results in bias in the coefficients of interest - unless their regressors are uncorrelated with the omitted ones. On the other hand, including any new variables increases (actual, not estimated) standard errors of other regressors. So we don't want to idly throw variables into the model. This lesson is about the second of these two issues, which is known as variance inflation.

We shall use simulations to illustrate variance inflation. The source code for these simulations is in a file named vifSims.R which I have copied into your working directory and tried to display in your source code editor. If I've failed to display it, you should open it manually.

Find the function, makelms, at the top of vifSims.R. The final expression in makelms creates 3 linear models. The first, lm(y ~ x1), predicts y in terms of x1, the second predicts y in terms of x1 and x2, the third in terms of all three regressors. The second coefficient of each model, for instance coef(lm(y ~ x1))[2], is extracted and returned in a 3-long vector. What does this second coefficient represent?

```{r}
"The coefficient of x1."
```

In makelms, the simulated dependent variable, y, depends on which of the regressors?

```{r}
"x1"
```

In vifSims.R, find the functions, rgp1() and rgp2(). Both functions generate 3 regressors, x1, x2, and x3. Compare the lines following the comment Point A in rgp1() with those following Point C in rgp2(). Which of the following statements about x1, x2, and x3 is true?

```{r}
"x1, x2, and x3 are uncorrelated in rgp1(), but not in rgp2()."
```

In the line following Point B in rgp1(), the function maklms(x1, x2, x3) is applied 1000 times. Each time it is applied, it simulates a new dependent variable, y, and returns estimates of the coefficient of x1 for each of the 3 models, y ~ x1, y ~ x1 + x2, and y ~ x1 + x2 + x3. It thus computes 1000 estimates of the  3 coefficients, collecting the results in 3x1000 array, beta. In the next line, the expression, apply(betas, 1, var), does which of the following?

```{r}
"Computes the variance of each row."
```

The function rgp1() computes the variance in estimates of the coefficient of x1 in each of the three models, y ~ x1, y ~ x1 + x2, and y ~ x1 + x2 + x3. (The results are rounded to 5 decimal places for convenient viewing.) This simulation approximates the variance (i.e., squared standard error) of x1's coefficient in each of these three models. Recall that variance inflation is due to correlated regressors and that in rgp1() the regressors are uncorrelated. Run the simulation rgp1() now. Be patient. It takes a while.

```{r}
makelms <- function(x1, x2, x3){
    # Simulate a dependent variable, y, as x1
    # plus a normally distributed error of mean 0 and 
    # standard deviation .3.
    y <- x1 + rnorm(length(x1), sd = .3)
    # Find the coefficient of x1 in 3 nested linear
    # models, the first including only the predictor x1,
    # the second x1 and x2, the third x1, x2, and x3.
    c(coef(lm(y ~ x1))[2], 
      coef(lm(y ~ x1 + x2))[2], 
      coef(lm(y ~ x1 + x2 + x3))[2])
}

# Regressor generation process 1.
rgp1 <- function(){
    print("Processing. Please wait.")
    # number of samples per simulation
    n <- 100
    # number of simulations
    nosim <- 1000
    # set seed for reproducibility
    set.seed(4321)
    # Point A
    x1 <- rnorm(n)
    x2 <- rnorm(n)
    x3 <- rnorm(n)
    # Point B
    betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
    round(apply(betas, 1, var), 5)
}
```

```{r}
rgp1()
```

The variances in each of the three models are approximately equal, as expected, since the other regressors, x2 and x3, are uncorrelated with the regressor of interest, x1. However, in rgp2(), x2 and x3 both depend on x1, so we should expect an effect. From the expressions assigning x2 and x3 which follow Point C, which is more strongly correlated with x1?

```{r}
"x3"
```

Run rgp2() to simulate standard errors in the coefficient of x1 for cases in which x1 is correlated with the other regressors

```{r}
# Regressor generation process 2.
rgp2 <- function(){
    print("Processing. Please wait.")
    # number of samples per simulation
    n <- 100
    # number of simulations
    nosim <- 1000
    # set seed for reproducibility
    set.seed(4321)
    # Point C
    x1 <- rnorm(n)
    x2 <- x1/sqrt(2) + rnorm(n) /sqrt(2)
    x3 <- x1 * 0.95 + rnorm(n) * sqrt(1 - 0.95^2)
    # Point D
    betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
    round(apply(betas, 1, var), 5)
}
```

```{r}
rgp2()
```

In this case, variance inflation due to correlated regressors is clear, and is most pronounced in the third model, y ~ x1 + x2 + x3, since x3 is the regressor most strongly correlated with x1.

In these two simulations we had 1000 samples of estimated coefficients, hence could calculate sample variance in order to illustrate the effect. In a real case, we have only one set of coefficients and we depend on theoretical estimates. However, theoretical estimates contain an unknown constant of proportionality. We therefore depend on ratios of theoretical estimates called Variance Inflation Factors, or VIFs.

A variance inflation factor (VIF) is a ratio of estimated variances, the variance due to including the ith regressor, divided by that due to including a corresponding ideal regressor which is uncorrelated with the others. VIF's can be calculated directly, but the car package provides a convenient method for the purpose as we will illustrate using the Swiss data from the datasets package.

According to its documentation, the Swiss data set consists of a standardized fertility measure and socioeconomic indicators for each of 47 French-speaking provinces of Switzerland in about 1888 when Swiss fertility rates began to fall. Type head(swiss) or View(swiss) to examine the data.

```{r}
head(swiss)
```

Fertility was thought to depend on five socioeconomic factors: the percent of males working in Agriculture, the percent of draftees receiving the highest grade on the army's Examination, the percent of draftees with Education beyond primary school, the percent of the population which was Roman Catholic, and the rate of Infant Mortality in the province. Use linear regression to model Fertility in terms of these five regressors and an intercept. Store the model in a variable named mdl.

```{r}
mdl <- lm(Fertility ~ ., swiss)
```

Calculate the VIF's for each of the regressors using vif(mdl).

```{r}
library("car")
vif(mdl)
```

These VIF's show, for each regression coefficient, the variance inflation due to including all the others. For instance, the variance in the estimated coefficient of Education is 2.774943 times what it might have been if Education were not correlated with the other regressors. Since Education and score on an Examination are likely to be correlated, we might guess that most of the variance inflation for Education is due to including Examination.

Make a second linear model of Fertility in which Examination is omitted, but the other four regressors are included. Store the result in a variable named mdl2.

```{r}
mdl2 <- lm(Fertility ~ . -Examination, swiss)
```

Calculate the VIF's for this model using vif(mdl2).

```{r}
vif(mdl2)
```

As expected, omitting Examination has markedly decreased the VIF for Education, from 2.774943 to 1.816361. Note that omitting Examination has had almost no effect the VIF for Infant Mortality. Chances are Examination and Infant Mortality are not strongly correlated. Now, before finishing this lesson, let's review several significant points.

A VIF describes the increase in the variance of a coefficient due to the correlation of its regressor with the other regressors. What is the relationship of a VIF to the standard error of its coefficient?

```{r}
"VIF is the square of standard error inflation."
```

If a regressor is strongly correlated with others, hence will increase their VIF's, why shouldn't we just exclude it?

```{r}
"Excluding it might bias coefficient estimates of regressors with which it is correlated."
```

The problems of variance inflation and bias due to excluded regressors both involve correlated regressors. However there are methods, such as factor analysis or principal componenent analysis, which can convert regressors to an equivalent uncorrelated set. Why then, when modeling, should we not just use uncorrelated regressors and avoid all the trouble?

```{r}
"Using converted regressors may make interpretation difficult."
```