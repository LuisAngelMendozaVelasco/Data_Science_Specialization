---
title: "swirl Lesson 2: Overfitting and Underfitting"
output: html_notebook
---

Overfitting and Underfitting. (Slides for this and other Data Science courses may be found at github https://github.com/DataScienceSpecialization/courses. If you care to use them, they must be downloaded as a zip file and viewed locally. This lesson corresponds to Regression_Models/02_04_residuals_variation_diagnostics.)

The Variance Inflation Factors lesson demonstrated that including new variables will increase standard errors of coefficient estimates of other, correlated regressors. Hence, we don't want to idly throw variables into the model. On the other hand, omitting variables results in bias in coefficients of regressors which are correlated with the omitted ones. In this lesson we demonstrate the effect of omitted variables and discuss the use of ANOVA to construct parsimonious, interpretable representations of the data.

First, I would like to illustrate how omitting a correlated regressor can bias estimates of a coefficient. The relevant source code is in a file named fitting.R which I have copied into your working directory and tried to display in your source code editor. If I've failed to display it, you should open it manually.

Find the function simbias() at the top of fitting.R. Below the comment labeled Point A three regressors, x1, x2, and x3, are defined. Which of these two are correlated?

```{r}
"x1 and x2"
```

Within simbias() another function, f(n), is defined. It forms a dependent variable, y, and at Point C returns the coefficient of x1 as estimated by two models, y ~ x1 + x2, and y ~ x1 + x3. One regressor is missing in each model. In the expression for y (Point B,) what is the actual coefficient of x1?
  
```{r}
"1"
```

At Point D in simbias() the internal function, f(), is applied 150 times and the results returned as a 2x150 matrix. The first row of this matrix contains independent estimates of x1's coefficient in the case that x3, the regressor uncorrelated with x1, is omitted. The second row contains estimates of x1's coefficient when the correlated regressor, x2, is omitted. Use simbias(), accepting the default argument, to form these estimates and store the result in a variable called x1c. (The default argument just guarantees a nice histogram, in a figure to follow.)

```{r}
simbias <- function(seed=8765){
    # The default seed guarantees a nice histogram. This is the only
    # reason that accepting the default, x1c <- simbias(), is required in the lesson. 
    # The effect will be evident with other seeds as well.
    set.seed(seed) 
    temp <- rnorm(100)
    # Point A
    x1 <- (temp + rnorm(100))/sqrt(2)
    x2 <- (temp + rnorm(100))/sqrt(2)
    x3 <- rnorm(100)
    # Function to simulate regression of y on 2 variables.
    f <- function(k){
        # Point B
        y <- x1 + x2 + x3 + .3*rnorm(100)
        # Point C
        c(lm(y ~ x1 + x2)$coef[2], lm(y ~ x1 + x3)$coef[2])
    }
    # Point D
    sapply(1:150, f)
}
```

```{r}
x1c <- simbias()
```

The actual coefficient of x1 is 1. Having been warned that omitting a correlated regressor would bias estimates of x1's coefficient, we would expect the mean estimate of x1c's second row to be farther from 1 than the mean of x1c's first row. Using apply(x1c, 1, mean), find the means of each row.

```{r}
apply(x1c, 1, mean)
```

Histograms of estimates from x1c's first row (blue) and second row (red) are shown. Estimates from the second row are clearly more than two standard deviations from the correct value of 1, and the bias due to omitting the correlated regressor is evident. (The code which produced this figure is incidental to the lesson, but is available as the function x1hist(), at the bottom of fitting.R.)

```{r}
# Plot histograms illustrating bias in estimates of a regressor
# coefficient 1) when an uncorrelated regressor is missing and
# 2) when a correlated regressor is missing.
x1hist <- function(x1c){
    p1 <- hist(x1c[1,], plot=FALSE)
    p2 <- hist(x1c[2,], plot=FALSE)
    yrange <- c(0, max(p1$counts, p2$counts))
    plot(p1, col=rgb(0, 0, 1, 1/4), xlim=range(x1c), ylim=yrange, xlab="Estimated coefficient of x1", main="Bias Effect of Omitted Regressor")
    plot(p2, col=rgb(1, 0, 0, 1/4), xlim=range(x1c), ylim=yrange, add=TRUE)
    legend(1.1, 40, c("Uncorrelated regressor, x3, omitted", "Correlated regressor, x2, omitted"), fill=c(rgb(0, 0, 1, 1/4), rgb(1, 0, 0, 1/4)))
}
```

```{r}
x1hist(x1c)
```

Adding even irrelevant regressors can cause a model to tend toward a perfect fit. We illustrate this by adding random regressors to the swiss data and regressing on progressively more of them. As the number of regressors approaches the number of data points (47), the residual sum of squares, also known as the deviance, approaches 0. (The source code for this figure can be found as function bogus() in fitting.R.

```{r}
# Illustrate the effect of bogus regressors on residual squared error.
bogus <- function(){
    temp <- swiss
    # Add 41 columns of random regressors to a copy of the swiss data.
    for(n in 1:41){temp[,paste0("random", n)] <- rnorm(nrow(temp))}
    # Define a function to compute the deviance of Fertility regressed
    # on all regressors up to column n. The function, deviance(model), computes
    # the residual sum of squares of the model given as its argument.
    f <- function(n){deviance(lm(Fertility ~ ., temp[,1:n]))}
    # Apply f to data from n=6, i.e., the legitimate regressors,
    # through n=47, i.e., a full complement of bogus regressors.
    rss <- sapply(6:47, f)
    # Display result.
    plot(0:41, rss, xlab="Number of bogus regressors.", ylab="Residual squared error.",
       main="Residual Squared Error for Swiss Data\nUsing Irrelevant (Bogus) Regressors",
       pch=21, bg='red')
}
```

```{r}
bogus()
```

In the figure, adding random regressors decreased deviance, but we would be mistaken to believe that such decreases are significant. To assess significance, we should take into account that adding regressors reduces residual degrees of freedom. Analysis of variance (ANOVA) is a useful way to quantify the significance of additional regressors. To exemplify its use, we will use the swiss data.

Recall that the Swiss data set consists of a standardized fertility measure and socioeconomic indicators for each of 47 French-speaking provinces of Switzerland in 1888. Fertility was thought to depend on an intercept and five factors denoted as Agriculture, Examination, Education, Catholic, and Infant Mortality. To begin our ANOVA example, regress Fertility on Agriculture and store the result in a variable named fit1.

```{r}
fit1 <- lm(Fertility ~ Agriculture, swiss)
```

Create another model, named fit3, by regressing Fertility on Agriculture and two additonal regressors, Examination and Education.

```{r}
fit3 <- lm(Fertility ~ Agriculture + Examination + Education, swiss)
```

We'll now use anova to assess the significance of the two added regressors. The null hypothesis is that the added regressors are not significant. We'll explain in detail shortly, but right now just apply the significance test by entering anova(fit1, fit3).

```{r}
anova(fit1, fit3)
```

The three asterisks, ***, at the lower right of the printed table indicate that the null hypothesis is rejected at the 0.001 level, so at least one of the two additional regressors is significant. Rejection is based on a right-tailed F test, Pr(>F), applied to an F value. According to the table, what is that F value?

```{r}
"20.968"
```

An F statistic is a ratio of two sums of squares divided by their respective degrees of freedom. If the two scaled sums are independent and centrally chi-squared distributed with the same variance, the statistic will have an F distribution with parameters given by the two degrees of freedom. In our case, the two sums are residual sums of squares which, as we know, have mean zero hence are centrally chi-squared provided the residuals themselves are normally distributed. The two relevant sums are given in the RSS (Residual Sum of Squares) column of the table. What are they?

```{r}
"6283.1 and 3180.9"
```

R's function, deviance(model), calculates the residual sum of squares, also known as the deviance, of the linear model given as its argument. Using deviance(fit3), verify that 3180.9 is fit3's residual sum of squares. (Of course, fit3 is called Model 2 in the table.)

```{r}
deviance(fit3)
```

In the next several steps, we will show how to calculate the F value, 20.968, which appears in the table printed by anova(). We'll begin with the denominator, which is fit3's residual sum of squares divided by its degrees of freedom. Fit3 has 43 residual degrees of freedom. This figure is obtained by subtracting 4, the the number of fit3's predictors (the 3 named and the intercept,) from 47, the number of samples in swiss. Store the value of deviance(fit3)/43 in a variable named d.

```{r}
d <- deviance(fit3)/43
```

The numerator is the difference, deviance(fit1)-deviance(fit3), divided by the difference in the residual degrees of freedom of fit1 and fit3, namely 2. This calculation requires some theoretical justification which we omit, but the essential idea is that fit3 has 2 predictors in addition to those of fit1. Calculate the numerator and store it in a variable named n.

```{r}
n <- (deviance(fit1) - deviance(fit3))/2
```

Calculate the ratio, n/d, to show it is essentially equal to the F value, 20.968, given by anova().

```{r}
n/d
```

We'll now calculate the p-value, which is the probability that a value of n/d or larger would be drawn from an F distribution which has parameters 2 and 43. This value was given as 4.407e-07 in the column labeled Pr(>F) in the table printed by anova(), a very unlikely value if the null hypothesis were true. Calculate this p-value using pf(n/d, 2, 43, lower.tail=FALSE).

```{r}
pf(n/d, 2, 43, lower.tail=FALSE)
```

Based on the calculated p-value, a false rejection of the null hypothesis is extremely unlikely. We are confident that fit3 is significantly better than fit1, with one caveat: analysis of variance is sensitive to its assumption that model residuals are approximately normal. If they are not, we could get a small p-value for that reason. It is thus worth testing residuals for normality. The Shapiro-Wilk test is quick and easy in R. Normality is its null hypothesis. Use shapiro.test(fit3$residuals) to test the residual of fit3.

```{r}
shapiro.test(fit3$residuals)
```

The Shapiro-Wilk p-value of 0.336 fails to reject normality, supporting confidence in our analysis of variance. In order to illustrate the use of anova() with more than two models, I have constructed fit5 and fit6 using the first 5 and all 6 regressors (including the intercept) respectively. Thus fit1, fit3, fit5, and fit6 form a nested sequence of models; the regressors of one are included in those of the next. Enter anova(fit1, fit3, fit5, fit6) at the R prompt now to get the flavor.

```{r}
fit5 <- lm(Fertility ~ Agriculture + Examination + Education + Catholic, swiss)
fit6 <- lm(Fertility ~ ., swiss)
```

```{r}
anova(fit1, fit3, fit5, fit6)
```

It appears that each model is a significant improvement on its predecessor. Before ending the lesson, let's review a few salient points.

Omitting a regressor can bias estimation of the coefficient of certain other regressors. Which ones?

```{r}
"Correlated regressors"
```

Including more regressors will reduce a model's residual sum of squares, even if the new regressors are irrelevant. True or False?

```{r}
"True"
```

When adding regressors, the reduction in residual sums of squares should be tested for significance above and beyond that of reducing residual degrees of freedom. R's anova() function uses an F-test for this purpose. What else should be done to insure that anova() applies?

```{r}
"Model residuals should be tested for normality."
```

That completes the lesson on underfitting and overfitting.