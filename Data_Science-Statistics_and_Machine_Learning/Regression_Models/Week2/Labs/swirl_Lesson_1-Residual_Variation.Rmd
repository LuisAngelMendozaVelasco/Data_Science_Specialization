---
title: "swirl Lesson 1: Residual Variation"
output: html_notebook
---

Residual Variation. (Slides for this and other Data Science courses may be found at github https://github.com/DataScienceSpecialization/courses. If you care to use them, they must be downloaded as a zip file and viewed locally. This lesson corresponds to Regression_Models/01_06_residualVariation. Galton data is from John Verzani's website, http://wiener.math.csi.cuny.edu/UsingR/)

As shown in the slides, residuals are useful for indicating how well data points fit a statistical model. They "can be thought of as the outcome (Y) with the linear association of the predictor (X) removed. One differentiates residual variation (variation after removing the predictor) from systematic variation (variation explained by the regression model)."

It can also be shown that, given a model, the maximum likelihood estimate of the variance of the random error is the average squared residual. However,  since our linear model with one predictor requires two parameters we have only (n-2) degrees of freedom. Therefore, to calculate an "average" squared residual to estimate the variance we use the formula 1/(n-2) * (the sum of the squared residuals). If we divided the sum of the squared residuals by n, instead of n-2, the result would give a biased estimate.

To see this we'll use our favorite Galton height data. First regenerate the regression line and call it fit. Use the R function lm and recall that by default its first argument is a formula such as "child ~ parent" and its second is the dataset, in this case galton. 

```{r}
galton <- read.csv("./data/galton.csv")
```

```{r}
fit <- lm(child ~ parent, galton)
```

First, we'll use the residuals (fit$residuals) of our model to estimate the standard deviation (sigma) of the error. We've already defined n for you as the number of points in Galton's dataset (928).

Calculate the sum of the squared residuals divided by the quantity (n-2).  Then take the square root.

```{r}
n <- 928
```

```{r}
sqrt(sum(fit$residuals^2) / (n - 2))
```

Now look at the "sigma" portion of the summary of fit, "summary(fit)$sigma".

```{r}
summary(fit)$sigma 
```

Pretty cool,  huh? 

Another cool thing - take the sqrt of  "deviance(fit)/(n-2)" at the R prompt.

```{r}
sqrt(deviance(fit) / (n-2))
```

Another useful fact shown in the slides was

Total Variation = Residual Variation + Regression Variation

Recall the beauty of the  slide full of algebra which proved this fact. It had a bunch of Y's, some with hats and some with bars and several summations of squared values. The Y's with hats were the estimates provided by the model. (They were on the regression line.) The Y with the bar was the mean or average of the data. Which sum of squared term represented Total Variation?

```{r}
"Yi-mean(Yi)"
```

Which sum of squared term represents Residual Variation?

```{r}
"Yi-Yi_hat"
```

The term R^2 represents the percent of total variation described by the model, the regression variation (the term we didn't ask about in the preceding multiple choice questions). Also, since it is a percent we need a ratio or fraction of sums of squares. Let's do this now for our Galton data.

We'll start with easy steps. Calculate the mean of the children's heights and store it in a variable called mu. Recall that we reference the childrens' heights with the expression 'galton$child' and the parents' heights with the expression 'galton$parent'.

```{r}
mu <- mean(galton$child) 
```

Recall that centering data means subtracting the mean from each data point. Now calculate the sum of the squares of the centered children's heights  and store the result in a variable called sTot. This represents the Total Variation of the data.

```{r}
sTot <- sum((galton$child-mu)^2)
```

Now create the variable sRes. Use the R function deviance to calculate the sum of the squares of the residuals. These are the distances between the children's heights and the regression line. This represents the Residual Variation. 

```{r}
sRes <- deviance(fit)
```

Finally, the ratio sRes/sTot represents the percent of total variation contributed by the residuals. To find the percent contributed by the model, i.e., the regression variation,  subtract the fraction sRes/sTot from 1.  This is the value R^2.

```{r}
1-sRes/sTot
```

For fun you can compare your result to the values shown in summary(fit)$r.squared to see if it looks familiar. Do this now.

```{r}
summary(fit)$r.squared
```

To see some real magic, compute the square of the correlation of the galton data, the children and parents. Use the R function cor.

```{r}
cor(galton$parent,galton$child)^2
```

We'll now summarize useful facts about R^2. It is the percentage of variation explained by the regression model. As a percentage it is between 0 and 1. It also equals the sample correlation squared. However, R^2 doesn't tell the whole story. 